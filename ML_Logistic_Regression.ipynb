{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# THEORY QUESTIONS\n",
        "\n",
        "1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "Ans -\n",
        "\n",
        "LINEAR REGRESSION\n",
        "\n",
        "1. Linear Regression is a supervised regression model.\n",
        "\n",
        "2. Equation of linear regression:\n",
        "\n",
        "(a0 + a1x1 + a2x2 + .... + aixi)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "Here,\n",
        "y = response variable\n",
        "\n",
        "xi = ith predictor variable\n",
        "\n",
        "ai= average effect on y as xi increases by 1\n",
        "\n",
        "3. In Linear Regression, we predict the value by an integer number.\n",
        "\n",
        "4. Here no activation function is used.\n",
        "\n",
        "5. Here no threshold value is needed.\n",
        "\n",
        "6. Here we calculate Root Mean Square Error(RMSE) to predict the next weight value.\n",
        "\n",
        "\n",
        "LOGISTIC REGRESSION\n",
        "\n",
        "1. Logistic Regression is a supervised classification model.\n",
        "\n",
        "2. Equation of logistic regression\n",
        "   \n",
        "     y (x) = e ^ (a0 + a1x1 + a2x2 + ... + aixi) / 1 + e ^ (a0 + a1x1 + a2x2 + ...+ aixi)\n",
        "\n",
        "Here,\n",
        "y = response variable\n",
        "\n",
        "xi = ith predictor variable\n",
        "\n",
        "ai = average effect on y as xi increases by 1\n",
        "\n",
        "3. In Logistic Regression, we predict the value by 1 or 0.\n",
        "\n",
        "4. Here activation function is used to convert a linear regression equation to the logistic regression equation\n",
        "\n",
        "5. Here a threshold value is added.\n",
        "\n",
        "6. Here we use precision to predict the next weight value.\n"
      ],
      "metadata": {
        "id": "atEqQVM3TxAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the mathematical equation of Logistic Regression?\n",
        "\n",
        "Ans -"
      ],
      "metadata": {
        "id": "r9ckTq1yXjql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Why do we use the Sigmoid function in Logistic Regression?\n",
        "\n",
        "Ans - It is easier to interpret. The sigmoid function is a smooth, continuous function, which makes it easier to understand the relationship between the input and output of the model. It is more common. The sigmoid function is more commonly used in logistic regression than other functions."
      ],
      "metadata": {
        "id": "qEskgjtiYQtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  What is the cost function of Logistic Regression?\n",
        "\n",
        "Ans - In logistic regression, the cost function, often called the log loss or cross-entropy loss, measures how well the model's predictions match the actual outcomes. It's calculated by averaging the loss over all training examples.\n",
        "\n",
        "The cost function (J) for logistic regression is typically represented as:\n",
        "\n",
        "J(w, b) = (1/m) * Σ ( -y(i) * log(hθ(x(i))) - (1 - y(i)) * log(1 - hθ(x(i))) )\n",
        "\n",
        "Where:\n",
        "\n",
        "m = the number of training examples\n",
        "\n",
        "y(i) = the actual outcome (0 or 1) for the i-th training example\n",
        "\n",
        "hθ(x(i)) = the model's predicted probability (between 0 and 1) for the i-th training example, given by the sigmoid function\n",
        "\n",
        "w = the model's parameters (weights)\n",
        "\n",
        "b = the model's bias"
      ],
      "metadata": {
        "id": "aGuTBSiNYlaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        "Ans - Regularization in logistic regression is a technique used to prevent overfitting by adding a penalty term to the cost function during training. This penalty discourages overly complex models and encourages simpler models that generalize better to unseen data. Without regularization, logistic regression, especially with high-dimensional data, can learn noise and outliers from the training set, leading to poor performance on new data.\n",
        "\n",
        "*Why is regularization needed?*\n",
        "\n",
        "1. Overfitting:\n",
        "\n",
        "Logistic regression, like other models, can overfit the training data, meaning it performs very well on the training set but poorly on new, unseen data. This happens when the model learns the noise and specific details of the training data instead of the underlying patterns.\n",
        "\n",
        "2. High Dimensionality:\n",
        "\n",
        "In datasets with many features, logistic regression can easily become overly complex, leading to overfitting.\n",
        "\n",
        "3. Generalization:\n",
        "\n",
        "Regularization helps the model generalize better to new data by simplifying its structure and reducing its reliance on specific training data points.\n",
        "\n",
        "4. Bias-Variance Tradeoff:\n",
        "\n",
        "Regularization helps to find a balance between bias and variance. By adding a penalty, it increases bias (making the model simpler) but reduces variance (making it less sensitive to the training data).\n",
        "\n",
        "5. Feature Importance:\n",
        "\n",
        "Regularization can also help in identifying the most important features by shrinking the coefficients of less important features, potentially driving them to zero in some cases (L1 regularization)."
      ],
      "metadata": {
        "id": "2QD038HEZA3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "\n",
        "Ans -\n",
        "\n",
        "LASSO REGRESSION\n",
        "\n",
        "1. Penalty Type - L1 Penalty: Lasso uses the absolute values of coefficients.\n",
        "\n",
        "2. Effect on Coefficients - It completely removes unnecessary features by setting their coefficients to zero.\n",
        "\n",
        "3. Best Use Case - It is best when we want to remove irrelevant features.\n",
        "\n",
        "4. Hyperparameters involved - Alpha: Controls how much regularization is applied. A higher alpha means more shrinkage.\n",
        "\n",
        "5. Bias and Variance - High bias, low variance\n",
        "\n",
        "RIDGE REGRESSION\n",
        "\n",
        "1. Penalty Type - L2 Penalty: Ridge uses the square of the coefficients.\n",
        "\n",
        "2. Effect on Coefficients - It makes all coefficients smaller but doesn’t set them to zero.\n",
        "\n",
        "3. Best Use Case - It is good when all features matter but we want to reduce their impact\n",
        "\n",
        "4. Hyperparameters involoved - Alpha: Similar to Lasso which helps in controlling the strength of regularization.\n",
        "\n",
        "5. Bias and Variance - Low bias, high variance\n",
        "\n",
        "ELASTIC NET REGRESSION\n",
        "\n",
        "1. Penalty Type - L1 + L2 Penalty: Elastic Net uses both absolute and square penalties together.\n",
        "\n",
        "2. Effect on Coefficients - It removes some features and reduces others by balancing both.\n",
        "\n",
        "3. Best Use Case - It is best for when we features are correlated and feature selection is needed\n",
        "\n",
        "4. Hyperparameters involved - Alpha + L1_ratio: Two parameters. Alpha controls regularization strength and L1_ratio adjusts the balance between Lasso and Ridge.\n",
        "\n",
        "5. Bias and Variance - Balance of bias and variance"
      ],
      "metadata": {
        "id": "dn1y_UvGZsMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "\n",
        "Ans - Elastic Net regression is a good choice when dealing with datasets that have multicollinearity (correlated features) and when feature selection is important, as it balances the benefits of both Lasso and Ridge regression. It's particularly useful when you have many features and a smaller number of samples.\n",
        "\n",
        "In summary, Elastic Net is a powerful tool when you need a combination of feature selection and handling multicollinearity, while Lasso and Ridge are more suitable when those are the primary goals."
      ],
      "metadata": {
        "id": "aBSu3oeCftjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "\n",
        "Ans - In Logistic Regression, the regularization parameter, lambda (λ), controls the strength of regularization applied to the model. Higher λ values lead to stronger regularization, which shrinks coefficients towards zero, simplifying the model and potentially reducing overfitting. Conversely, lower λ values result in weaker regularization, allowing the model to fit the training data more closely, but potentially leading to overfitting.\n"
      ],
      "metadata": {
        "id": "nRO7sPAagFgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the key assumptions of Logistic Regression?\n",
        "\n",
        "Ans - Logistic regression, a statistical model for binary classification, relies on several key assumptions. These assumptions, when met, ensure the model's validity and reliability. The main assumptions are: binary outcome, independence of observations, linearity of log-odds, and absence of multicollinearity. Additionally, a sufficiently large sample size is crucial for robust results."
      ],
      "metadata": {
        "id": "onvBV6TugSgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  What are some alternatives to Logistic Regression for classification tasks?\n",
        "\n",
        "Ans -\n",
        "1. Support Vector Machines (SVM):\n",
        "\n",
        "SVMs are powerful algorithms that work by finding the optimal hyperplane to separate data points into different classes. They are effective with both linear and non-linear data, especially when dealing with high-dimensional data.\n",
        "\n",
        "2. Decision Trees:\n",
        "\n",
        "Decision trees build a tree-like structure to classify data based on a series of rules. They are relatively easy to interpret and can handle both categorical and numerical data.\n",
        "\n",
        "3. k-Nearest Neighbors (k-NN):\n",
        "\n",
        "This algorithm classifies data points based on the majority class of their k nearest neighbors in the feature space. It is simple to implement and effective for certain types of data.\n",
        "\n",
        "4. Naive Bayes:\n",
        "\n",
        "This probabilistic classifier assumes independence between features and is computationally efficient. It is often a good starting point for classification tasks."
      ],
      "metadata": {
        "id": "4V13TOeXgvGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  What are Classification Evaluation Metrics?\n",
        "\n",
        "Ans - Common classification evaluation metrics include accuracy, precision, recall, F1-score, AUC-ROC, and the confusion matrix. These metrics help assess how well a classification model distinguishes between different classes.\n",
        "\n",
        "-> Accuracy -\n",
        "\n",
        "The proportion of correctly classified instances out of the total instances. It's a good general measure, but can be misleading with imbalanced datasets (where one class has significantly more instances than others).\n",
        "\n",
        "-> Precision -\n",
        "\n",
        "Out of all instances predicted as positive, what proportion is actually positive? High precision indicates fewer false positives.\n",
        "\n",
        "-> F1-Score -\n",
        "\n",
        "The harmonic mean of precision and recall. It provides a balanced measure that considers both false positives and false negatives, especially useful when there's an imbalance between classes.\n",
        "\n",
        "-> AUC-ROC -\n",
        "\n",
        "The area under the Receiver Operating Characteristic curve. It measures the model's ability to distinguish between classes at various threshold settings, particularly useful for binary classification problems.\n",
        "\n",
        "-> Confusion Matrix -\n",
        "\n",
        "A table that summarizes the performance of a classification model by showing the counts of true positives, true negatives, false positives, and false negatives."
      ],
      "metadata": {
        "id": "VpdQXjzjhQpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.  How does class imbalance affect Logistic Regression?\n",
        "\n",
        "Ans - Class imbalance in logistic regression can lead to a biased model that favors the majority class, resulting in poor performance, especially in predicting the minority class. This bias arises because logistic regression aims to maximize overall accuracy, treating all classes equally. When one class is significantly larger than others, the model tends to classify instances as belonging to the majority class to achieve higher accuracy scores, even if it means misclassifying a large portion of the minority class."
      ],
      "metadata": {
        "id": "Zi55ogC4hzQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "Ans - Hyperparameter tuning in logistic regression is the process of finding the optimal values for the hyperparameters of the model, which are parameters that are set before training and influence the learning process. These hyperparameters affect the model's performance, complexity, and learning speed. By tuning these hyperparameters, you can improve the model's accuracy and generalization capabilities.\n"
      ],
      "metadata": {
        "id": "cZliNTR2iA2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.  What are different solvers in Logistic Regression? Which one should be used?\n",
        "\n",
        "Ans - In Logistic Regression, different solvers refer to the optimization algorithms used to find the best model parameters. The choice of solver depends on factors like the size and complexity of your dataset, the type of regularization used, and whether you're dealing with binary or multiclass classification problems.\n",
        "\n",
        "Which solver to use-\n",
        "\n",
        "-> For small datasets: Use liblinear.\n",
        "\n",
        "-> For medium datasets: lbfgs is a good default choice.\n",
        "\n",
        "-> For large datasets: sag or saga are recommended for their efficiency.\n",
        "\n",
        "-> For multiclass problems: All solvers except liblinear can handle these problems. liblinear can only handle binary classification unless used with OneVsRestClassifier."
      ],
      "metadata": {
        "id": "x6GhAm3HiMCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "Ans - Logistic regression, originally designed for binary classification, can be extended to handle multiclass problems using several strategies. The most common are One-vs-Rest (OvR), One-vs-One (OvO), and Multinomial Logistic Regression (also known as Softmax Regression). These methods adapt the binary logistic regression framework to classify data into multiple categories.\n",
        "\n",
        "1. *One-vs-Rest (OvR) or One-vs-All (OvA)*\n",
        "\n",
        "This strategy trains a separate binary logistic regression model for each class, where one class is treated as the positive class and all other classes are combined into a single negative class.\n",
        "\n",
        "For example, if you have three classes (A, B, and C), you would train three models: A vs. (B+C), B vs. (A+C), and C vs. (A+B).\n",
        "During prediction, each model outputs a probability, and the class with the highest probability is chosen.\n",
        "\n",
        "2. . *One-vs-One (OvO)*\n",
        "\n",
        "In OvO, a binary logistic regression model is trained for every possible pair of classes.\n",
        "\n",
        "If you have N classes, you'll train N*(N-1)/2 models.\n",
        "For prediction, each model \"votes\" for one of the two classes it's trained on, and the class with the most votes is selected.\n",
        "\n",
        "3. *Multinomial Logistic Regression (Softmax Regression)*\n",
        "\n",
        "This method directly extends binary logistic regression to handle multiple classes within a single model.\n",
        "\n",
        "It uses the softmax function to output probabilities for each class, ensuring that the probabilities sum up to 1.\n",
        "This approach is more efficient than OvR or OvO, especially for a large number of classes, as it trains a single model instead of multiple."
      ],
      "metadata": {
        "id": "X0mI8JMvimUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the advantages and disadvantages of Logistic Regression?\n",
        "\n",
        "Ans -\n",
        "\n",
        "*ADVANTAGES*\n",
        "\n",
        "1. Logistic regression is easier to implement, interpret, and very efficient to train.\n",
        "\n",
        "2. It makes no assumptions about distributions of classes in feature space.\n",
        "\n",
        "3. It can easily extend to multiple classes(multinomial regression) and a natural probabilistic view of class predictions.\n",
        "\n",
        "4. It not only provides a measure of how appropriate a predictor(coefficient size)is, but also its direction of association (positive or negative).\n",
        "\n",
        "5. It is very fast at classifying unknown records.\n",
        "\n",
        "6. Good accuracy for many simple data sets and it performs well when the dataset is linearly separable.\n",
        "\n",
        "\n",
        "*DISADVANTAGES*\n",
        "\n",
        "1. If the number of observations is lesser than the number of features, Logistic Regression should not be used, otherwise, it may lead to overfitting.\n",
        "\n",
        "2. It constructs linear boundaries.\n",
        "\n",
        "3. The major limitation of Logistic Regression is the assumption of linearity between the dependent variable and the independent variables.\n",
        "\n",
        "4. It can only be used to predict discrete functions. Hence, the dependent variable of Logistic Regression is bound to the discrete number set.\n",
        "\n",
        "5. Non-linear problems can't be solved with logistic regression because it has a linear decision surface. Linearly separable data is rarely found in real-world scenarios.\n",
        "\n",
        "6. Logistic Regression requires average or no multicollinearity between independent variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "RTvTvVW7jjo3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.  What are some use cases of Logistic Regression?\n",
        "\n",
        "Ans - Logistic regression is used when the outcome variable is categorical, specifically when it has two or more discrete outcomes. It's a classification algorithm that estimates the probability of an instance belonging to a particular category.\n",
        "\n",
        "-> Medical Diagnosis: Predicting whether a patient has a disease based on various health indicators.\n",
        "\n",
        "-> Fraud Detection: Identifying fraudulent transactions in banking or insurance.\n",
        "\n",
        "-> Marketing: Predicting customer churn or the likelihood of a customer purchasing a product.\n",
        "\n",
        "-> Spam Filtering: Classifying emails as spam or not spam.\n",
        "\n",
        "-> Risk Assessment: Evaluating the risk of loan defaults or other potential losses.\n",
        "\n",
        "-> Natural Language Processing: Analyzing text data to determine sentiment or categorize documents.\n"
      ],
      "metadata": {
        "id": "rogEqEWtljln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "Ans -\n",
        "\n",
        "*Logistic Regression (Binary Classification)*\n",
        "\n",
        "-> Purpose: Used for binary classification — i.e., problems with only two classes (e.g., spam or not spam).\n",
        "\n",
        "-> Output: Produces a single probability value between 0 and 1 using the sigmoid function.\n",
        "\n",
        "-> Decision Rule: If probability > 0.5 → class 1, else class 0.\n",
        "\n",
        "-> Equation\n",
        "\n",
        "    P (y = 1|x) = σ (w ^ T x + b) = 1 / 1 + e ^ -(w^tx + b)\n",
        "\n",
        "*Softmax Regression(Multinomial Logistic Regression)*\n",
        "\n",
        "-> Purpose: Used for multi-class classification — i.e., problems with three or more classes (e.g., classifying digits 0–9).\n",
        "\n",
        "-> Output: Produces a probability distribution across all classes using the softmax function.\n",
        "\n",
        "-> Decision Rule: Choose the class with the highest probability.\n",
        "\n",
        "-> Equation\n",
        "\n",
        "   P (y = k|x) = e ^ w ^ T k x + bk / ∑ ^ K j = 1   e ^ w ^ Tj x + bi\n",
        "\n",
        "   for each class k\n"
      ],
      "metadata": {
        "id": "csimKudTpWMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "Ans - Choosing between One-vs-Rest (OvR) and Softmax (a.k.a. Multinomial Logistic Regression) depends on a few factors like model interpretability, performance, data distribution, and training time.\n",
        "\n",
        "-> OvR (One-vs-Rest)\tTrains one binary classifier per class (e.g., class A vs not A) and selects the class with the highest score.\n",
        "\n",
        "-> Softmax\tTrains a single model that directly estimates the probabilities across all classes using the softmax function.\n",
        "\n",
        "*when to use OvR*\n",
        "\n",
        "-> You have many classes (e.g., 50+).\n",
        "\n",
        "-> Your classes are imbalanced (some are rare).\n",
        "\n",
        "-> You want model simplicity and faster training for large-scale data.\n",
        "\n",
        "-> You're using algorithms that don't naturally support multiclass, like SVMs or binary decision trees.\n",
        "\n",
        "*when to use Softmax*\n",
        "\n",
        "-> You have fewer classes (e.g., <20) and sufficient data.\n",
        "\n",
        "-> You want a probability distribution over all classes.\n",
        "\n",
        "-> You need calibrated probabilities (for decision making or ranking).\n",
        "\n",
        "-> You're working with neural networks or deep learning models."
      ],
      "metadata": {
        "id": "Q0YyCHwIsEOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "Ans - In logistic regression, coefficients represent the change in the log-odds of the outcome variable for a one-unit change in the predictor variable, holding other predictors constant. To interpret these coefficients, they are typically exponentiated to obtain odds ratios. An odds ratio greater than 1 indicates a positive association, meaning higher values of the predictor are associated with increased odds of the outcome. Conversely, an odds ratio less than 1 indicates a negative association, meaning higher values of the predictor are associated with decreased odds of the outcome."
      ],
      "metadata": {
        "id": "BeMnu_DutcaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PRACTICAL QUESTIONS\n",
        "\n",
        "1.  Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy"
      ],
      "metadata": {
        "id": "jSvIgZqBtvOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy pandas scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkHKjhLhuOJ3",
        "outputId": "38108b21-4992-41d1-8756-27d3e72d786e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the model accuracy\n",
        "print(f'Model Accuracy: {accuracy:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_dfVg5MvLY8",
        "outputId": "6ea1a492-6dce-470b-88a3-46d02d99a2af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "3DB3C8flvW7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy scikit-learn pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DFvR93svais",
        "outputId": "8572ac0f-a213-4659-d514-e9a3d267946a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize L1-regularized Logistic Regression (Lasso)\n",
        "# Note: solver must be 'liblinear' or 'saga' for L1 penalty\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy (L1 Regularization): {accuracy:.4f}')\n",
        "\n",
        "# Print model coefficients (for feature importance analysis)\n",
        "print(\"\\nCoefficients (after L1 regularization):\")\n",
        "for i, coef in enumerate(model.coef_[0]):\n",
        "    print(f\"Feature {i+1}: {coef:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb1avCZtvs3a",
        "outputId": "3a36604c-134c-4c1a-ea7a-9a516348480f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy (L1 Regularization): 1.0000\n",
            "\n",
            "Coefficients (after L1 regularization):\n",
            "Feature 1: 0.0000\n",
            "Feature 2: 2.3648\n",
            "Feature 3: -2.6741\n",
            "Feature 4: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients."
      ],
      "metadata": {
        "id": "2Kq-i5FgvyN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize L2-regularized Logistic Regression (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy (L2 Regularization): {accuracy:.4f}')\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"\\nCoefficients (after L2 regularization):\")\n",
        "for i, coef in enumerate(model.coef_[0]):\n",
        "    print(f\"Feature {i+1}: {coef:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vK3D7CTmv7jc",
        "outputId": "75ee3f31-e9b9-4fea-f05b-5cd2d148bb64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy (L2 Regularization): 1.0000\n",
            "\n",
            "Coefficients (after L2 regularization):\n",
            "Feature 1: -0.3935\n",
            "Feature 2: 0.9625\n",
            "Feature 3: -2.3751\n",
            "Feature 4: -0.9987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')"
      ],
      "metadata": {
        "id": "hN_htaKtwFVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Elastic Net Regularized Logistic Regression\n",
        "# l1_ratio = 0.5 means equal contribution from L1 and L2\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy (Elastic Net Regularization): {accuracy:.4f}')\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"\\nCoefficients (after Elastic Net regularization):\")\n",
        "for i, coef in enumerate(model.coef_[0]):\n",
        "    print(f\"Feature {i+1}: {coef:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnWfjcWrwRYf",
        "outputId": "10e4f22e-3b02-4b53-fc3a-f73f325d631c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy (Elastic Net Regularization): 1.0000\n",
            "\n",
            "Coefficients (after Elastic Net regularization):\n",
            "Feature 1: 0.3888\n",
            "Feature 2: 1.7744\n",
            "Feature 3: -2.4239\n",
            "Feature 4: -0.7043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'"
      ],
      "metadata": {
        "id": "oiZ8zPp1wqNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression with OvR strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=200, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy (One-vs-Rest): {accuracy:.4f}')\n",
        "\n",
        "# Print classification report for detailed metrics\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"\\nCoefficients (for each class):\")\n",
        "for i, coef in enumerate(model.coef_):\n",
        "    print(f\"Class {iris.target_names[i]}: {coef}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tFf5GlMxIDP",
        "outputId": "b6864974-d797-407f-d48b-ac436e2ae11e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy (One-vs-Rest): 0.9667\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.89      0.94         9\n",
            "   virginica       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n",
            "\n",
            "Coefficients (for each class):\n",
            "Class setosa: [-0.42762216  0.88771927 -2.21471658 -0.91610036]\n",
            "Class versicolor: [-0.03387836 -2.0442989   0.54266011 -1.0179372 ]\n",
            "Class virginica: [-0.38904645 -0.62147609  2.7762982   2.09067085]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy"
      ],
      "metadata": {
        "id": "-mzq09UrxZRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],  # Types of regularization\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the best parameters and accuracy\n",
        "print(f'Best Parameters: {best_params}')\n",
        "print(f'Best Cross-Validation Accuracy: {best_score:.4f}')\n",
        "print(f'Test Set Accuracy: {test_accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5futkMeAxm6V",
        "outputId": "3c2d9c36-15ea-409d-91eb-618ba489d4b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 1, 'penalty': 'l2'}\n",
            "Best Cross-Validation Accuracy: 0.9667\n",
            "Test Set Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "50 fits failed out of a total of 75.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "25 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "25 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan 0.85833333        nan        nan 0.93333333        nan\n",
            "        nan 0.96666667        nan        nan 0.94166667        nan\n",
            "        nan 0.95              nan]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy."
      ],
      "metadata": {
        "id": "-d8NE37xxxPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, random_state=42)\n",
        "\n",
        "# Initialize Stratified K-Fold with 5 splits\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# List to store accuracy for each fold\n",
        "accuracies = []\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Calculate the average accuracy\n",
        "average_accuracy = np.mean(accuracies)\n",
        "\n",
        "# Print the average accuracy\n",
        "print(f'Average Accuracy (Stratified K-Fold): {average_accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12W_jP-yx9DR",
        "outputId": "60422bf4-07cd-4ea5-890f-8b1a0dcf6315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy (Stratified K-Fold): 0.9667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy."
      ],
      "metadata": {
        "id": "x075MJth0Bue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset from a CSV file\n",
        "# Replace 'your_dataset.csv' with the path to your CSV file\n",
        "data = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"Dataset Preview:\")\n",
        "print(data.head())\n",
        "\n",
        "# Assuming the last column is the target variable and the rest are features\n",
        "X = data.iloc[:, :-1]  # Features (all columns except the last)\n",
        "y = data.iloc[:, -1]   # Target variable (last column)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f'Model Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "9F6-W39X0IUU",
        "outputId": "2c0d0b27-1094-4f31-9833-8dcaa417d37b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'your_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-11-1156022565.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the dataset from a CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Replace 'your_dataset.csv' with the path to your CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'your_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Display the first few rows of the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy"
      ],
      "metadata": {
        "id": "5buRmaRy0ScY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, random_state=42)\n",
        "\n",
        "# Define the parameter distribution for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'C': np.logspace(-4, 4, 20),  # Regularization strength\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],  # Types of regularization\n",
        "    'solver': ['liblinear', 'saga']  # Solvers that support L1 and ElasticNet\n",
        "}\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist,\n",
        "                                   n_iter=50, cv=5, scoring='accuracy', random_state=42)\n",
        "\n",
        "# Fit RandomizedSearchCV\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = random_search.best_params_\n",
        "best_score = random_search.best_score_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the best parameters and accuracy\n",
        "print(f'Best Parameters: {best_params}')\n",
        "print(f'Best Cross-Validation Accuracy: {best_score:.4f}')\n",
        "print(f'Test Set Accuracy: {test_accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DhxAaQB0Zyj",
        "outputId": "35137b5b-6980-472a-b388-9bed08b99737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'solver': 'saga', 'penalty': 'l1', 'C': np.float64(0.615848211066026)}\n",
            "Best Cross-Validation Accuracy: 0.9750\n",
            "Test Set Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "85 fits failed out of a total of 250.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1203, in fit\n",
            "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
            "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "50 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.83333333        nan        nan 0.975      0.65833333        nan\n",
            " 0.96666667        nan        nan        nan 0.33333333 0.94166667\n",
            "        nan 0.49166667        nan 0.96666667 0.96666667 0.33333333\n",
            "        nan 0.96666667        nan 0.95       0.33333333        nan\n",
            " 0.68333333 0.33333333 0.76666667 0.95              nan 0.96666667\n",
            " 0.33333333 0.95833333        nan 0.95       0.95833333 0.96666667\n",
            " 0.325             nan        nan 0.66666667 0.875      0.33333333\n",
            " 0.96666667        nan 0.96666667 0.96666667        nan 0.95833333\n",
            " 0.95       0.90833333]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy."
      ],
      "metadata": {
        "id": "guK7wvuW0qET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "logistic_model = LogisticRegression(max_iter=200, random_state=42)\n",
        "\n",
        "# Initialize One-vs-One Classifier with Logistic Regression\n",
        "ovo_classifier = OneVsOneClassifier(logistic_model)\n",
        "\n",
        "# Train the OvO classifier\n",
        "ovo_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ovo_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f'One-vs-One Multiclass Logistic Regression Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A9OpVY70vNv",
        "outputId": "99efa028-f19c-460a-d1a1-8df7e0782a82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One Multiclass Logistic Regression Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification"
      ],
      "metadata": {
        "id": "zAIhEHMK081Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "90fG46Cp1DMP",
        "outputId": "5a326606-60c7-455b-a110-9553e156b9e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.8550\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score"
      ],
      "metadata": {
        "id": "NcMNQpYC1Sb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_test, y_pred, target_names=['Negative', 'Positive'])\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJmHw_To1Xp1",
        "outputId": "39a2e30d-5f8a-4938-b0c9-5a8ea0cbbd8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.8550\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.80      0.91      0.85        93\n",
            "    Positive       0.91      0.80      0.86       107\n",
            "\n",
            "    accuracy                           0.85       200\n",
            "   macro avg       0.86      0.86      0.85       200\n",
            "weighted avg       0.86      0.85      0.86       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.  Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance"
      ],
      "metadata": {
        "id": "9zUBKWC42H_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Generate a synthetic imbalanced binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,\n",
        "                           weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with class weights\n",
        "model = LogisticRegression(max_iter=200, random_state=42, class_weight='balanced')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_test, y_pred, target_names=['Negative', 'Positive'])\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51T4qLy92QGX",
        "outputId": "5e617e7b-c274-456a-f80b-4104cc853f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.8650\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.97      0.88      0.92       180\n",
            "    Positive       0.41      0.75      0.53        20\n",
            "\n",
            "    accuracy                           0.86       200\n",
            "   macro avg       0.69      0.81      0.72       200\n",
            "weighted avg       0.91      0.86      0.88       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.  Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance"
      ],
      "metadata": {
        "id": "TMAMmt7p2cdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load the Titanic dataset\n",
        "# Make sure to have the Titanic dataset CSV file in the same directory or provide the correct path\n",
        "titanic_data = pd.read_csv('titanic.csv')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"Dataset Preview:\")\n",
        "print(titanic_data.head())\n",
        "\n",
        "# Handle missing values\n",
        "# Fill missing 'Age' with the median age\n",
        "titanic_data['Age'].fillna(titanic_data['Age'].median(), inplace=True)\n",
        "\n",
        "# Drop the 'Cabin' column as it has too many missing values\n",
        "titanic_data.drop(columns=['Cabin'], inplace=True)\n",
        "\n",
        "# Fill missing 'Embarked' with the most common port (mode)\n",
        "titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "# Convert 'Sex' to numerical values (0 for female, 1 for male)\n",
        "titanic_data['Sex'] = titanic_data['Sex'].map({'female': 0, 'male': 1})\n",
        "\n",
        "# Convert 'Embarked' to numerical values (0, 1, 2)\n",
        "titanic_data = pd.get_dummies(titanic_data, columns=['Embarked'], drop_first=True)\n",
        "\n",
        "# Select features and target variable\n",
        "X = titanic_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']]\n",
        "y = titanic_data['Survived']\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_test, y_pred, target_names=['Not Survived', 'Survived'])\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "7xfMkFDs33Zy",
        "outputId": "7fd6f054-49ea-4af1-d87b-2c10f97a9721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'titanic.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-17-3635379586.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the Titanic dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Make sure to have the Titanic dataset CSV file in the same directory or provide the correct path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtitanic_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'titanic.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Display the first few rows of the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titanic.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling."
      ],
      "metadata": {
        "id": "Y6mP6Wzg4Kbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, random_state=42)\n",
        "\n",
        "# Train the model without scaling\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# Print accuracy without scaling\n",
        "print(f'Accuracy without scaling: {accuracy_no_scaling:.4f}')\n",
        "\n",
        "# Apply Standardization (Feature Scaling)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the model with scaling\n",
        "model.fit(X_train_scaled, y_train)\n",
        "y_pred_with_scaling = model.predict(X_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "# Print accuracy with scaling\n",
        "print(f'Accuracy with scaling: {accuracy_with_scaling:.4f}')\n",
        "\n",
        "# Generate classification reports\n",
        "report_no_scaling = classification_report(y_test, y_pred_no_scaling, target_names=['Negative', 'Positive'])\n",
        "report_with_scaling = classification_report(y_test, y_pred_with_scaling, target_names=['Negative', 'Positive'])\n",
        "\n",
        "print(\"\\nClassification Report without Scaling:\")\n",
        "print(report_no_scaling)\n",
        "\n",
        "print(\"\\nClassification Report with Scaling:\")\n",
        "print(report_with_scaling)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCnKiTTv4V46",
        "outputId": "1ac776e8-d97e-4865-f3f4-a99b72b48ff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.8550\n",
            "Accuracy with scaling: 0.8550\n",
            "\n",
            "Classification Report without Scaling:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.80      0.91      0.85        93\n",
            "    Positive       0.91      0.80      0.86       107\n",
            "\n",
            "    accuracy                           0.85       200\n",
            "   macro avg       0.86      0.86      0.85       200\n",
            "weighted avg       0.86      0.85      0.86       200\n",
            "\n",
            "\n",
            "Classification Report with Scaling:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.80      0.91      0.85        93\n",
            "    Positive       0.91      0.80      0.86       107\n",
            "\n",
            "    accuracy                           0.85       200\n",
            "   macro avg       0.86      0.86      0.85       200\n",
            "weighted avg       0.86      0.85      0.86       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score."
      ],
      "metadata": {
        "id": "vtMnl1zQ5MAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f'ROC-AUC Score: {roc_auc:.4f}')\n",
        "\n",
        "# Calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', label='ROC Curve (area = {:.2f})'.format(roc_auc))\n",
        "plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Diagonal line\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "_g8LNX9F5Qfx",
        "outputId": "dbb09721-161b-46ae-f0aa-ea8be17381b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9216\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy."
      ],
      "metadata": {
        "id": "XLjWDVPF5uF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model with custom learning rate (C=0.5)\n",
        "model = LogisticRegression(C=0.5, max_iter=200, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy with C=0.5: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZoGOFOx54_A",
        "outputId": "1924dffc-a0a2-4922-a239-c6dab19b0e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with C=0.5: 0.8550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train Logistic Regression and identify important features based on model coefficients."
      ],
      "metadata": {
        "id": "P7z37Noi7nLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
        "\n",
        "# Convert to DataFrame for better feature handling\n",
        "feature_names = [f'Feature_{i}' for i in range(X.shape[1])]\n",
        "X_df = pd.DataFrame(X, columns=feature_names)\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get the model coefficients\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Create a DataFrame to hold feature names and their corresponding coefficients\n",
        "feature_importance = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
        "\n",
        "# Sort the features by the absolute value of their coefficients\n",
        "feature_importance['Importance'] = np.abs(feature_importance['Coefficient'])\n",
        "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Display the important features\n",
        "print(\"Important Features based on Model Coefficients:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Optionally, display only the top N important features\n",
        "top_n = 10\n",
        "print(f\"\\nTop {top_n} Important Features:\")\n",
        "print(feature_importance.head(top_n))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKBtEpMF7tSP",
        "outputId": "39db9213-bc49-430b-efa3-701ccde39589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Important Features based on Model Coefficients:\n",
            "       Feature  Coefficient  Importance\n",
            "11  Feature_11     0.651270    0.651270\n",
            "14  Feature_14    -0.565964    0.565964\n",
            "2    Feature_2    -0.542849    0.542849\n",
            "15  Feature_15    -0.425930    0.425930\n",
            "17  Feature_17     0.377717    0.377717\n",
            "3    Feature_3    -0.289054    0.289054\n",
            "7    Feature_7     0.283839    0.283839\n",
            "5    Feature_5    -0.237133    0.237133\n",
            "18  Feature_18     0.171816    0.171816\n",
            "1    Feature_1     0.160185    0.160185\n",
            "0    Feature_0    -0.098185    0.098185\n",
            "19  Feature_19     0.090047    0.090047\n",
            "9    Feature_9    -0.076143    0.076143\n",
            "4    Feature_4    -0.066278    0.066278\n",
            "8    Feature_8     0.064172    0.064172\n",
            "16  Feature_16    -0.044079    0.044079\n",
            "10  Feature_10     0.031174    0.031174\n",
            "6    Feature_6     0.021846    0.021846\n",
            "12  Feature_12    -0.006081    0.006081\n",
            "13  Feature_13     0.001719    0.001719\n",
            "\n",
            "Top 10 Important Features:\n",
            "       Feature  Coefficient  Importance\n",
            "11  Feature_11     0.651270    0.651270\n",
            "14  Feature_14    -0.565964    0.565964\n",
            "2    Feature_2    -0.542849    0.542849\n",
            "15  Feature_15    -0.425930    0.425930\n",
            "17  Feature_17     0.377717    0.377717\n",
            "3    Feature_3    -0.289054    0.289054\n",
            "7    Feature_7     0.283839    0.283839\n",
            "5    Feature_5    -0.237133    0.237133\n",
            "18  Feature_18     0.171816    0.171816\n",
            "1    Feature_1     0.160185    0.160185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  Write a Python program to train Logistic Regression and evaluate its performance using Cohen's Kappa Score."
      ],
      "metadata": {
        "id": "9Jf551Z59RxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Calculate Cohen's Kappa Score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rsWeoYQ9asw",
        "outputId": "61798708-30e2-40f9-83e4-4e2045d923e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.8550\n",
            "Cohen's Kappa Score: 0.7112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification."
      ],
      "metadata": {
        "id": "SnayXEZi9uqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get predicted probabilities for the positive class\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate precision and recall\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Calculate average precision score\n",
        "average_precision = average_precision_score(y_test, y_scores)\n",
        "print(f'Average Precision Score: {average_precision:.4f}')\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='blue', label='Precision-Recall Curve')\n",
        "plt.fill_between(recall, precision, alpha=0.2, color='blue')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.axhline(0.5, color='red', linestyle='--')  # Reference line for precision\n",
        "plt.axvline(0.5, color='green', linestyle='--')  # Reference line for recall\n",
        "plt.legend(loc='lower left')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "YKJc6WMl-CCH",
        "outputId": "f0a577f7-6a4c-45f3-dea5-a943f79019d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Precision Score: 0.9394\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.  Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy."
      ],
      "metadata": {
        "id": "Y9eC2wRa-joa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List of solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "accuracies = {}\n",
        "\n",
        "# Train and evaluate Logistic Regression with different solvers\n",
        "for solver in solvers:\n",
        "    # Initialize the Logistic Regression model with the current solver\n",
        "    model = LogisticRegression(solver=solver, max_iter=200, random_state=42)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies[solver] = accuracy\n",
        "    print(f'Accuracy with solver \"{solver}\": {accuracy:.4f}')\n",
        "\n",
        "# Display the accuracies for all solvers\n",
        "print(\"\\nAccuracy Comparison:\")\n",
        "for solver, accuracy in accuracies.items():\n",
        "    print(f'Solver: {solver}, Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BSoB88A-rMy",
        "outputId": "a7570b00-014a-4b8a-deb1-4c37911fdcaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with solver \"liblinear\": 0.8550\n",
            "Accuracy with solver \"saga\": 0.8550\n",
            "Accuracy with solver \"lbfgs\": 0.8550\n",
            "\n",
            "Accuracy Comparison:\n",
            "Solver: liblinear, Accuracy: 0.8550\n",
            "Solver: saga, Accuracy: 0.8550\n",
            "Solver: lbfgs, Accuracy: 0.8550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.  Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)"
      ],
      "metadata": {
        "id": "paEb18wP-05s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef, accuracy_score\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f'Matthews Correlation Coefficient (MCC): {mcc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dn2J7ggm-9wk",
        "outputId": "c1200a33-0505-41b0-c4ee-0cbc4ed611f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.8550\n",
            "Matthews Correlation Coefficient (MCC): 0.7172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling"
      ],
      "metadata": {
        "id": "U9SK09z5_ViV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate on raw data\n",
        "lr_raw = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_raw.fit(X_train, y_train)\n",
        "raw_pred = lr_raw.predict(X_test)\n",
        "raw_acc = accuracy_score(y_test, raw_pred)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train and evaluate on standardized data\n",
        "lr_scaled = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_scaled.fit(X_train_scaled, y_train)\n",
        "scaled_pred = lr_scaled.predict(X_test_scaled)\n",
        "scaled_acc = accuracy_score(y_test, scaled_pred)\n",
        "\n",
        "# Print comparison results\n",
        "print(\"Accuracy Comparison:\")\n",
        "print(f\"Raw data accuracy: {raw_acc:.4f}\")\n",
        "print(f\"Standardized data accuracy: {scaled_acc:.4f}\")\n",
        "print(f\"Accuracy improvement: {scaled_acc - raw_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1pG9Mjf_Ztq",
        "outputId": "70de53db-0db1-4ce3-e086-1da9ce3e71ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Comparison:\n",
            "Raw data accuracy: 0.8550\n",
            "Standardized data accuracy: 0.8550\n",
            "Accuracy improvement: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.  Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation"
      ],
      "metadata": {
        "id": "umKyeNjv_oZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = LogisticRegression(max_iter=200, random_state=42)\n",
        "\n",
        "# Define the parameter grid for C\n",
        "param_grid = {'C': np.logspace(-4, 4, 20)}  # Testing values from 0.0001 to 10000\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_C = grid_search.best_params_['C']\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f'Optimal C: {best_C:.4f}')\n",
        "print(f'Best cross-validated accuracy: {best_score:.4f}')\n",
        "print(f'Test set accuracy with optimal C: {test_accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1GHzMTI_sXf",
        "outputId": "449183c7-0be6-422f-c5b4-be4b4177159c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal C: 0.0886\n",
            "Best cross-validated accuracy: 0.8700\n",
            "Test set accuracy with optimal C: 0.8650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.  Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions."
      ],
      "metadata": {
        "id": "_JTvmGWtAPuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Save the trained model using joblib\n",
        "model_filename = 'logistic_regression_model.joblib'\n",
        "joblib.dump(model, model_filename)\n",
        "print(f'Model saved to {model_filename}')\n",
        "\n",
        "# Load the model from the file\n",
        "loaded_model = joblib.load(model_filename)\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "loaded_y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy of the loaded model\n",
        "loaded_accuracy = accuracy_score(y_test, loaded_y_pred)\n",
        "print(f'Loaded Model Accuracy: {loaded_accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "ONkmZOWGASpP",
        "outputId": "fedd0ead-4eef-4f39-f0b7-38fd19c56746",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.8550\n",
            "Model saved to logistic_regression_model.joblib\n",
            "Loaded Model Accuracy: 0.8550\n"
          ]
        }
      ]
    }
  ]
}